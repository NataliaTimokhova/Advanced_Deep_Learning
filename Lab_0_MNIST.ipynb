{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the packeges\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.models as models\n",
    "from torchvision.models import AlexNet_Weights\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Train a CNN on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part, we build and train a simple CNN on MNIST. Then we use the trained network (its weights) as a starting point (i.e. as a pretrained model) for training on the SVHN dataset. In this transfer‐learning setup we “fine‑tune” the model on SVHN. Notice that because MNIST images are 28×28 grayscale and SVHN images are originally 32×32 RGB, we convert SVHN to grayscale and resize it to 28×28 so that the network architecture remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for MNIST\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download MNIST dataset\n",
    "train_dataset_mnist = MNIST(root='./MNIST', train=True, download=True, transform=transform_mnist)\n",
    "test_dataset_mnist = MNIST(root='./MNIST', train=False, download=True, transform=transform_mnist)\n",
    "\n",
    "train_loader_mnist = DataLoader(dataset=train_dataset_mnist, batch_size=64, shuffle=True)\n",
    "test_loader_mnist = DataLoader(dataset=test_dataset_mnist, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN for MNIST\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Input: 1 x 28 x 28\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 32 x 28 x 28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 32 x 14 x 14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 64 x 14 x 14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                              # 64 x 7 x 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)  # 10 classes for MNIST\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on MNIST...\n",
      "Epoch 1/5 - Loss: 0.1544, Accuracy: 95.30%\n",
      "Epoch 2/5 - Loss: 0.0469, Accuracy: 98.54%\n",
      "Epoch 3/5 - Loss: 0.0319, Accuracy: 98.99%\n",
      "Epoch 4/5 - Loss: 0.0226, Accuracy: 99.29%\n",
      "Epoch 5/5 - Loss: 0.0171, Accuracy: 99.45%\n",
      "\n",
      "MNIST Test Accuracy: 99.04%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the model on MNIST\n",
    "model_mnist = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mnist.parameters(), lr=0.001)\n",
    "num_epochs = 5  # use a small number for demonstration\n",
    "\n",
    "print(\"Training on MNIST...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model_mnist.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_mnist:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_mnist(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Evaluate on MNIST test set\n",
    "model_mnist.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_mnist:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_mnist(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "mnist_test_acc = 100 * correct / total\n",
    "print(f\"\\nMNIST Test Accuracy: {mnist_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because MNIST is a relatively simple and clean dataset, our CNN learns to recognize digits very effectively, achieving very high accuracy (around 99%). This training process establishes a strong baseline and creates a set of learned features that are specific to digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Transfer Learning from MNIST to SVHN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part, we use the MNIST-trained CNN as a starting point for the SVHN dataset. SVHN consists of more complex 32×32 RGB images of house numbers. To make these images compatible with our MNIST model, we convert them to grayscale and resize them to 28×28. Then, we fine-tune the pre-trained model on SVHN. This fine-tuning allows the model to adapt its learned features to the new, more challenging domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for SVHN: convert to grayscale, resize to 28x28, and normalize\n",
    "transform_svhn = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert RGB to grayscale\n",
    "    transforms.Resize(28),                          # Resize to 28x28\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# Download SVHN dataset (the train set here is used for fine-tuning)\n",
    "train_dataset_svhn = datasets.SVHN(root='./data', split='train', download=True, transform=transform_svhn)\n",
    "test_dataset_svhn = datasets.SVHN(root='./data', split='test', download=True, transform=transform_svhn)\n",
    "\n",
    "train_loader_svhn = DataLoader(dataset=train_dataset_svhn, batch_size=64, shuffle=True)\n",
    "test_loader_svhn = DataLoader(dataset=test_dataset_svhn, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning on SVHN...\n",
      "Epoch 1/5 - Loss: 0.6427, Accuracy: 81.09%\n",
      "Epoch 2/5 - Loss: 0.3873, Accuracy: 88.66%\n",
      "Epoch 3/5 - Loss: 0.3103, Accuracy: 90.87%\n",
      "Epoch 4/5 - Loss: 0.2567, Accuracy: 92.44%\n",
      "Epoch 5/5 - Loss: 0.2143, Accuracy: 93.81%\n",
      "\n",
      "SVHN Test Accuracy (Transfer Learning from MNIST): 89.77%\n"
     ]
    }
   ],
   "source": [
    "# Use the model pre-trained on MNIST as initialization.\n",
    "# Optionally, you can freeze some layers. Here, we'll fine-tune the whole network.\n",
    "model_svhn = SimpleCNN().to(device)\n",
    "model_svhn.load_state_dict(model_mnist.state_dict())  # load pre-trained MNIST weights\n",
    "\n",
    "# Define a new optimizer for fine-tuning on SVHN\n",
    "optimizer_svhn = optim.Adam(model_svhn.parameters(), lr=0.001)\n",
    "num_epochs_svhn = 5  # number of fine-tuning epochs\n",
    "\n",
    "print(\"\\nFine-tuning on SVHN...\")\n",
    "for epoch in range(num_epochs_svhn):\n",
    "    model_svhn.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_svhn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_svhn.zero_grad()\n",
    "        outputs = model_svhn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_svhn.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_svhn} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Evaluate on SVHN test set\n",
    "model_svhn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_svhn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_svhn(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "svhn_test_acc = 100 * correct / total\n",
    "print(f\"\\nSVHN Test Accuracy (Transfer Learning from MNIST): {svhn_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the transferred model starts with a good initialization (thanks to MNIST), its performance on SVHN is lower (around 89.77% test accuracy) due to the higher variability and complexity in SVHN images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Differences and Results\n",
    "\n",
    "- Dataset Complexity:\n",
    "MNIST is simpler, with centered, clean handwritten digits, leading to near-perfect performance. In contrast, SVHN is more diverse with real-world variations in color, background, and noise, making it a harder task.\n",
    "\n",
    "- Training Strategy:\n",
    "In Part 1, the model is fully trained on MNIST from scratch. In Part 2, we leverage transfer learning by fine-tuning the MNIST model on SVHN. This means the model uses the robust features learned on MNIST as a foundation but adapts them to handle the additional complexity in SVHN.\n",
    "\n",
    "- Results:\n",
    "The MNIST model achieves extremely high accuracy because the task is straightforward. The SVHN model, while benefiting from the transferred features, achieves lower accuracy due to the increased difficulty of the dataset. This demonstrates that transfer learning can be effective, but the performance is influenced by how similar the source and target domains are.\n",
    "\n",
    "Overall, transfer learning from MNIST to SVHN shows that while features learned on a simple dataset can help with a more complex one, the differences in image characteristics naturally lead to a drop in performance when moving to a more challenging task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_labb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
